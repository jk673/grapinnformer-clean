{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMVQnxSiSGagMdE+HbDkIQo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jk673/grapinnformer/blob/main/test_gnn_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages and modules"
      ],
      "metadata": {
        "id": "igQ6AujW67Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, sys, subprocess\n",
        "\n",
        "def in_colab():\n",
        "    try:\n",
        "        import google.colab  # Colab 전용 모듈\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# 버전 및 wheel URL 구성\n",
        "torch_ver = torch.__version__.split('+')[0]           # e.g. '2.8.0'\n",
        "cuda_ver  = torch.version.cuda or ''\n",
        "cu_tag    = f\"cu{cuda_ver.replace('.','')}\" if torch.cuda.is_available() and cuda_ver else \"cpu\"\n",
        "url = f\"https://data.pyg.org/whl/torch-{torch_ver}+{cu_tag}.html\"\n",
        "\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", cuda_ver or \"cpu\", \"| PYG wheel index:\", url)\n",
        "\n",
        "# 설치\n",
        "if in_colab():\n",
        "    !pip install pyvista\n",
        "    !pip install torch_geometric\n",
        "    subprocess.check_call([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\", \"-U\",\n",
        "        \"pyg-lib\", \"torch-scatter\", \"torch-sparse\", \"torch-cluster\", \"torch-spline-conv\",\n",
        "        \"-f\", url\n",
        "    ])\n",
        "\n",
        "print(\"✅ Done. Please restart runtime/kernel, then re-run your code.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ABedStj8G-K",
        "outputId": "66eacced-477f-4b35-fdfa-e6a079f9ac2b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 | CUDA: 12.6 | PYG wheel index: https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
            "Collecting pyvista\n",
            "  Downloading pyvista-0.46.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: matplotlib>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from pyvista) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from pyvista) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pyvista) (11.3.0)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from pyvista) (1.8.2)\n",
            "Requirement already satisfied: scooby>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from pyvista) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from pyvista) (4.14.1)\n",
            "Collecting vtk!=9.4.0 (from pyvista)\n",
            "  Downloading vtk-9.5.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.1->pyvista) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch->pyvista) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch->pyvista) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.1->pyvista) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->pyvista) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->pyvista) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->pyvista) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->pyvista) (2025.8.3)\n",
            "Downloading pyvista-0.46.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vtk-9.5.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (112.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vtk, pyvista\n",
            "Successfully installed pyvista-0.46.1 vtk-9.5.0\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "✅ Done. Please restart runtime/kernel, then re-run your code.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKJteyQ26tjt",
        "outputId": "e79db2ba-b6df-42df-9e48-67aff407df16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pyg-lib ok; 0.4.0+pt28cu126\n",
            "Torch: 2.8.0+cu126\n",
            "CUDA: 12.6\n",
            "PyVista: 0.46.1\n",
            "PyG: 2.6.1\n",
            "py  : 3.12.11\n"
          ]
        }
      ],
      "source": [
        "# 1. Environment & dependencies\n",
        "import gc\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "from itertools import chain\n",
        "from os import PathLike\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pyvista as pv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "from torch import Tensor\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Batch, Data\n",
        "from torch_geometric.loader import DataLoader, NeighborLoader\n",
        "from torch_geometric.nn import GCNConv, GraphNorm, JumpingKnowledge, TransformerConv\n",
        "from tqdm.auto import tqdm\n",
        "import wandb\n",
        "import pyg_lib\n",
        "\n",
        "# Data and model settings\n",
        "DATA_ROOT = Path('data')\n",
        "TARGET_FIELD = 'static(p)_coeffMean'\n",
        "USE_NORMALS = True\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print(\"pyg-lib ok;\", getattr(pyg_lib, \"__version__\", \"ok\"))\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA:', torch.version.cuda)\n",
        "print('PyVista:', pv.__version__)\n",
        "print('PyG:', torch_geometric.__version__ if 'torch_geometric' in sys.modules else 'unknown')\n",
        "print(\"py  :\", sys.version.split()[0])  # ex) 3.11.x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "nIv3cw4D7C9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FourierPosEnc(nn.Module):\n",
        "    def __init__(self, in_ch=3, num_frequencies=2):  # ★ pos 주파수 절반으로\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"freqs\", 2.0**torch.arange(num_frequencies) * torch.pi)\n",
        "        self.out_dim = in_ch*(2*num_frequencies)\n",
        "    def forward(self, pos):\n",
        "        pe = []\n",
        "        for f in self.freqs:\n",
        "            ang = f * pos\n",
        "            pe += [torch.sin(ang), torch.cos(ang)]\n",
        "        return torch.cat(pe, dim=-1)\n",
        "\n",
        "class FiLM(nn.Module):\n",
        "    def __init__(self, cond_dim, hidden):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(cond_dim, hidden*2), nn.GELU(), nn.Linear(hidden*2, hidden*2)\n",
        "        )\n",
        "    def forward(self, h, cond):\n",
        "        if cond is None: return h\n",
        "        if cond.dim()==1: cond = cond.unsqueeze(0)\n",
        "        gamma, beta = torch.chunk(self.mlp(cond), 2, dim=-1)\n",
        "        return h * (1 + gamma.squeeze(0)) + beta.squeeze(0)\n",
        "\n",
        "class BoundaryGraphNet(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=96, layers=4, dropout=0.15,   # ★ 192→96, 6→4\n",
        "                 heads=2, use_pos_enc=True, pos_freqs=2, cond_dim=4, # ★ 4→2\n",
        "                 out_dim_p=1, out_dim_tau=3, jk_mode='last', use_checkpoint=False,\n",
        "                 predict_tau=False):\n",
        "        super().__init__()\n",
        "        self.use_pos_enc = use_pos_enc\n",
        "        self.pos_enc = FourierPosEnc(3, pos_freqs) if use_pos_enc else None\n",
        "        self.predict_tau = predict_tau\n",
        "        pe_dim = self.pos_enc.out_dim if use_pos_enc else 0\n",
        "\n",
        "        self.x_enc = nn.Sequential(\n",
        "            nn.Linear(in_dim + pe_dim, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, hidden)\n",
        "        )\n",
        "\n",
        "        # ★ edge_dim을 8로 크게 축소\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(4, 16), nn.GELU(), nn.Linear(16, 8)\n",
        "        )\n",
        "        self.edge_dim = 8\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.norms  = nn.ModuleList()\n",
        "        for _ in range(layers):\n",
        "            # ★ concat=False, out_channels=hidden, add_self_loops=False\n",
        "            self.layers.append(TransformerConv(\n",
        "                in_channels=hidden,\n",
        "                out_channels=hidden,\n",
        "                heads=heads,\n",
        "                concat=False,\n",
        "                dropout=dropout,\n",
        "                edge_dim=self.edge_dim,\n",
        "                beta=True,\n",
        "            ))\n",
        "            self.norms.append(GraphNorm(hidden))\n",
        "\n",
        "        # ★ JK를 'last'로 기본 설정(메모리 절약). 필요시 'max'로 바꿔도 됨\n",
        "        self.jk_mode = jk_mode\n",
        "        if jk_mode == 'max':\n",
        "            self.jk = JumpingKnowledge(mode='max')\n",
        "        else:\n",
        "            self.jk = None\n",
        "\n",
        "        self.p_head   = nn.Sequential(nn.Linear(hidden, hidden), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden, out_dim_p))\n",
        "        self.tau_head = nn.Sequential(nn.Linear(hidden, hidden), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden, out_dim_tau))\n",
        "\n",
        "        self.film = FiLM(cond_dim, hidden) if cond_dim>0 else None\n",
        "        self.dropout = dropout\n",
        "        self.use_checkpoint = use_checkpoint  # ★ 필요할 때만 켜기\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_edge_attr(pos, edge_index):\n",
        "        i, j = edge_index\n",
        "        rij = pos[j] - pos[i]\n",
        "        dij = torch.norm(rij, dim=1, keepdim=True).clamp_min(1e-12)\n",
        "        return torch.cat([rij, dij], dim=1)  # [E,4]\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, pos, edge_index = data.x, data.pos, data.edge_index\n",
        "        pe = self.pos_enc(pos) if self.use_pos_enc else None\n",
        "        h = self.x_enc(torch.cat([x, pe], dim=-1) if pe is not None else x)\n",
        "\n",
        "        eattr = getattr(data, 'edge_attr', None)\n",
        "        if eattr is None:\n",
        "            eattr = self._make_edge_attr(pos, edge_index)\n",
        "        e = self.edge_encoder(eattr)\n",
        "\n",
        "        hs = [] if self.jk_mode == 'max' else None\n",
        "\n",
        "        for conv, norm in zip(self.layers, self.norms):\n",
        "            def block(h_in):\n",
        "                h_mid = conv(h_in, edge_index, edge_attr=e)\n",
        "                if self.film is not None and hasattr(data, 'global_cond'):\n",
        "                    h_mid = self.film(h_mid, data.global_cond)\n",
        "                h_mid = norm(h_mid)\n",
        "                h_mid = F.gelu(h_mid)\n",
        "                h_mid = F.dropout(h_mid, p=self.dropout, training=self.training)\n",
        "                return h_mid\n",
        "\n",
        "            h_res = h\n",
        "            # ★ 선택적 체크포인팅(메모리↓, 연산↑)\n",
        "            h = block(h) if not self.use_checkpoint else cp.checkpoint(block, h)\n",
        "            h = h + h_res\n",
        "            if hs is not None:\n",
        "                hs.append(h)\n",
        "\n",
        "        if self.jk_mode == 'max':\n",
        "            h = self.jk(hs)\n",
        "        # 'last'면 그냥 마지막 h 사용\n",
        "\n",
        "        p_pred   = self.p_head(h)\n",
        "        tau_pred = self.tau_head(h)\n",
        "        return (p_pred, tau_pred) if self.predict_tau else p_pred"
      ],
      "metadata": {
        "id": "-ey3l__57D0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "Ojeq6m5O7GJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_dim = 8\n",
        "target_dim = 1\n",
        "inferred_cond_dim = 2\n",
        "\n",
        "# --------- 하이퍼파라미터 ---------\n",
        "BATCH_SIZE   = 1\n",
        "LR           = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS       = 75\n",
        "PRINT_EVERY  = 5\n",
        "GRAD_CLIP    = 5.0\n",
        "\n",
        "# surface-only loss 가중치\n",
        "LOSS_WEIGHTS = {\"data\": 1.0, \"tv\": 0.05, \"lap\": 0.01}\n",
        "USE_TV = True\n",
        "EPS = 1e-12\n",
        "\n",
        "# --------- device ----------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "\n",
        "# --------- 모델 정의/이동 ----------\n",
        "# ⚠️ out_dim을 타겟 채널 수로!\n",
        "model = BoundaryGraphNet(\n",
        "    in_dim=in_dim,\n",
        "    hidden=256,\n",
        "    layers=4,\n",
        "    dropout=0.1,\n",
        "    heads=2,\n",
        "    use_pos_enc=True,\n",
        "    pos_freqs=2,\n",
        "    cond_dim=4   # 예: [Uinf, rho, nu, A_ref]를 data.global_cond로 넣을 때\n",
        ").to(device)\n",
        "\n",
        "# --------- optimizer (+옵션: 스케줄러) ----------\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=LR*0.1)\n",
        "\n",
        "# --------- W&B init ----------\n",
        "os.environ[\"WANDB_DISABLE_SYMLINKS\"] = \"true\"  # <- wandb.init 이전에!\n",
        "\n",
        "wandb_run = wandb.init(\n",
        "    project=\"gnn-pinn-250819\",     # <- 프로젝트명 바꿔도 됨\n",
        "    name=f\"surface_only_{int(time.time())}\",\n",
        "    config={\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"lr\": LR,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"grad_clip\": GRAD_CLIP,\n",
        "        \"loss_weights\": LOSS_WEIGHTS,\n",
        "        \"use_tv\": USE_TV,\n",
        "        \"feat_dim\": in_dim,\n",
        "        \"target_dim\": target_dim,\n",
        "        \"model\": model.__class__.__name__,\n",
        "        \"optimizer\": \"Adam\",\n",
        "    },\n",
        ")\n",
        "# 과도한 비용 피하려면 watch는 주석처리 가능\n",
        "# wandb.watch(model, log=\"all\", log_freq=50)\n",
        "\n",
        "# --------- 기하 도우미/손실 ---------\n",
        "def edge_geo_terms(pos: Tensor, edge_index: Tensor):\n",
        "    i, j = edge_index\n",
        "    e_ij   = pos[j] - pos[i]\n",
        "    len_ij = e_ij.norm(dim=1, keepdim=True).clamp_min(torch.finfo(pos.dtype).tiny)\n",
        "    t_ij   = e_ij / len_ij\n",
        "    return i, j, t_ij, len_ij\n",
        "\n",
        "def graph_edge_weights(pos: Tensor, edge_index: Tensor, mode=\"invlen\", clamp=10.0):\n",
        "    _, _, _, len_ij = edge_geo_terms(pos, edge_index)\n",
        "    if mode == \"invlen\":\n",
        "        w = (1.0 / (len_ij + EPS)).squeeze(1)\n",
        "        if clamp is not None:\n",
        "            w = w.clamp_max(clamp)\n",
        "        return w\n",
        "    else:\n",
        "        return torch.ones(edge_index.size(1), device=pos.device, dtype=pos.dtype)\n",
        "\n",
        "def edge_tv_or_l2(y_pred: Tensor, edge_index: Tensor, edge_w: Tensor, use_tv=True):\n",
        "    i, j = edge_index\n",
        "    diff = y_pred[i] - y_pred[j]  # (E,C)\n",
        "    if use_tv:\n",
        "        return (diff.abs() * edge_w.unsqueeze(-1)).mean()\n",
        "    else:\n",
        "        return ((diff**2) * edge_w.unsqueeze(-1)).mean()\n",
        "\n",
        "def laplacian_reg(y_pred, edge_index, num_nodes, edge_w=None, eps=1e-12):\n",
        "    \"\"\"\n",
        "    y_pred: [N, C]\n",
        "    edge_index: [2, E] (long)\n",
        "    edge_w: [E] or None\n",
        "    \"\"\"\n",
        "    # --- 준비: dtype/device 정렬 ---\n",
        "    device = y_pred.device\n",
        "    dtype  = torch.float32  # 라플라시안은 fp32로 계산 권장\n",
        "    # (원래 y_pred가 fp16/ bf16여도 여기선 fp32로 올려 계산 후 다시 캐스팅)\n",
        "\n",
        "    # 인덱스는 long, 같은 디바이스\n",
        "    i, j = edge_index\n",
        "    i = i.to(device=device, dtype=torch.long)\n",
        "    j = j.to(device=device, dtype=torch.long)\n",
        "\n",
        "    E = i.numel()\n",
        "    C = y_pred.size(1)\n",
        "\n",
        "    # 엣지 가중치\n",
        "    if edge_w is None:\n",
        "        edge_w = torch.ones(E, device=device, dtype=dtype)\n",
        "    else:\n",
        "        edge_w = edge_w.to(device=device, dtype=dtype)\n",
        "\n",
        "    # y를 fp32로\n",
        "    y = y_pred.to(dtype)\n",
        "\n",
        "    # --- degree 및 가중합 ---\n",
        "    deg = torch.zeros(num_nodes, device=device, dtype=dtype)\n",
        "    deg.scatter_add_(0, i, edge_w)  # deg[u] = sum_v w_uv\n",
        "\n",
        "    wyj = torch.zeros((num_nodes, C), device=device, dtype=dtype)\n",
        "    wyj.scatter_add_(0, i.unsqueeze(-1).expand(-1, C),\n",
        "                     (edge_w.unsqueeze(-1) * y[j]))\n",
        "\n",
        "    # 라플라시안 L y = D y - W y  (여기선 행 정규화/대칭 정규화 없이 단순형)\n",
        "    Ly = deg.unsqueeze(-1) * y - wyj\n",
        "\n",
        "    # 규제값: ||Ly||^2 (노드/채널 평균)\n",
        "    reg = (Ly.pow(2).sum(dim=1)).mean()\n",
        "\n",
        "    # 원래 dtype으로 캐스팅해서 리턴\n",
        "    return reg.to(y_pred.dtype)\n",
        "\n",
        "\n",
        "def surface_only_loss(batch, pred: Tensor, loss_weights=LOSS_WEIGHTS):\n",
        "    x = batch.x\n",
        "    y = batch.y\n",
        "    edge_index = batch.edge_index\n",
        "    pos = batch.pos if hasattr(batch, 'pos') and batch.pos is not None else x[:, :3]\n",
        "\n",
        "    data_loss = nn.functional.mse_loss(pred, y)\n",
        "    w_e = graph_edge_weights(pos, edge_index, mode=\"invlen\")\n",
        "\n",
        "    tv_loss  = edge_tv_or_l2(pred, edge_index, w_e, use_tv=USE_TV)\n",
        "\n",
        "    if hasattr(batch, 'batch') and batch.batch is not None:\n",
        "        lap_loss_acc = 0.0\n",
        "        uniq = batch.batch.unique()\n",
        "        for g_id in uniq:\n",
        "            mask = (batch.batch == g_id)\n",
        "            node_idx = torch.nonzero(mask, as_tuple=False).squeeze(1)\n",
        "            mask_i = mask[edge_index[0]]\n",
        "            mask_j = mask[edge_index[1]]\n",
        "            e_mask = mask_i & mask_j\n",
        "            if e_mask.sum() == 0:\n",
        "                continue\n",
        "            sub_e = edge_index[:, e_mask]\n",
        "            old2new = -torch.ones(mask.size(0), device=mask.device, dtype=torch.long)\n",
        "            old2new[node_idx] = torch.arange(node_idx.size(0), device=mask.device)\n",
        "            sub_e = old2new[sub_e]\n",
        "            lap_loss_acc = lap_loss_acc + laplacian_reg(pred[mask], sub_e, node_idx.size(0), w_e[e_mask])\n",
        "        lap_loss = lap_loss_acc / (uniq.numel() + EPS)\n",
        "    else:\n",
        "        lap_loss = laplacian_reg(pred, edge_index, x.size(0), w_e)\n",
        "\n",
        "    loss = (loss_weights[\"data\"] * data_loss +\n",
        "            loss_weights[\"tv\"]   * tv_loss  +\n",
        "            loss_weights[\"lap\"]  * lap_loss)\n",
        "\n",
        "    return loss, {\"loss_data\": data_loss.detach(),\n",
        "                  \"loss_tv\":   tv_loss.detach(),\n",
        "                  \"loss_lap\":  lap_loss.detach()}\n",
        "\n",
        "# ======================\n",
        "# Training / Validation\n",
        "# ======================\n",
        "best_val = float('inf')\n",
        "best_path = \"best_surface_only.pt\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_iter = chain(*(iter(ld) for ld in train_loader))\n",
        "\n",
        "    model.train()\n",
        "    tr_loss_sum = 0.0; tr_nodes = 0\n",
        "    ep_data = ep_tv = ep_lap = 0.0\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "\n",
        "    for step, batch in enumerate(train_iter, start=1):\n",
        "        batch = batch.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "            pred = model(batch)\n",
        "            if pred.dim() == 1:\n",
        "                pred = pred.unsqueeze(1)\n",
        "            loss, parts = surface_only_loss(batch, pred)\n",
        "\n",
        "\n",
        "        # ★ 올바른 순서: backward -> unscale -> clip -> step -> update\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        if GRAD_CLIP and GRAD_CLIP > 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # ★ CosineAnnealingWarmRestarts는 iteration 단위 step이 자연스러움\n",
        "        scheduler.step(epoch - 1 + step / len_epoch)\n",
        "\n",
        "        n = batch.x.size(0)\n",
        "        tr_loss_sum += loss.item() * n\n",
        "        tr_nodes    += n\n",
        "        ep_data += parts[\"loss_data\"].item() * n\n",
        "        ep_tv   += parts[\"loss_tv\"].item()   * n\n",
        "        ep_lap  += parts[\"loss_lap\"].item()  * n\n",
        "\n",
        "    train_loss = tr_loss_sum / max(tr_nodes, 1)\n",
        "    train_data = ep_data / max(tr_nodes, 1)\n",
        "    train_tv   = ep_tv   / max(tr_nodes, 1)\n",
        "    train_lap  = ep_lap  / max(tr_nodes, 1)\n",
        "\n",
        "    # -------- Validation --------\n",
        "    model.eval()\n",
        "    va_loss_sum = 0.0\n",
        "    va_nodes    = 0\n",
        "    va_data, va_tv, va_lap = 0.0, 0.0, 0.0\n",
        "\n",
        "    val_iter = chain(*(iter(ld) for ld in val_loader))\n",
        "    with torch.no_grad():\n",
        "        for batch in val_iter:\n",
        "            batch = batch.to(device, non_blocking=True)\n",
        "            pred = model(batch)\n",
        "            if pred.dim() == 1:\n",
        "                pred = pred.unsqueeze(1)\n",
        "            v_loss, v_parts = surface_only_loss(batch, pred)\n",
        "\n",
        "            n = batch.x.size(0)\n",
        "            data_loss = v_parts[\"loss_data\"]           # 데이터 MSE만 사용\n",
        "            va_loss_sum += data_loss.item() * n   # total 대신 data loss만 누적\n",
        "            va_nodes    += n\n",
        "\n",
        "            va_data += v_parts[\"loss_data\"].item() * n\n",
        "            va_tv   += v_parts[\"loss_tv\"].item()   * n\n",
        "            va_lap  += v_parts[\"loss_lap\"].item()  * n\n",
        "\n",
        "    val_loss = va_loss_sum / max(va_nodes, 1)\n",
        "    val_data = va_data / max(va_nodes, 1)\n",
        "    val_tv   = va_tv   / max(va_nodes, 1)\n",
        "    val_lap  = va_lap  / max(va_nodes, 1)\n",
        "\n",
        "    # -------- W&B logging --------\n",
        "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"lr\": current_lr,\n",
        "        \"train/loss\": train_loss,\n",
        "        \"train/loss_data\": train_data,\n",
        "        \"train/loss_tv\": train_tv,\n",
        "        \"train/loss_lap\": train_lap,\n",
        "        \"val/loss\": val_loss,\n",
        "        \"val/loss_data\": val_data,\n",
        "        \"val/loss_tv\": val_tv,\n",
        "        \"val/loss_lap\": val_lap,\n",
        "    })\n",
        "\n",
        "    if epoch % PRINT_EVERY == 0 or epoch == 1:\n",
        "\n",
        "        print(f\"[{epoch:03d}] train {train_loss:.6f} | val {val_loss:.6f} | lr {current_lr:.2e}\")\n",
        "\n",
        "        # -------- best model 저장 & W&B 업로드 --------\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            torch.save({\"model\": model.state_dict(),\n",
        "                         \"optimizer\": optimizer.state_dict(),\n",
        "                         \"epoch\": epoch,\n",
        "                         \"val_loss\": best_val}, best_path)\n",
        "            # \"config\": wandb.config}, best_path)\n",
        "            # 파일을 W&B에 첨부 (Artifacts가 필요 없으면 이걸로 충분)\n",
        "            artifact = wandb.Artifact(\n",
        "            name=\"best_model\",\n",
        "            type=\"model\",\n",
        "            metadata={\"val_loss\": float(best_val)}\n",
        "            )\n",
        "            artifact.add_file(best_path, name=\"best_surface_only.pt\")\n",
        "            wandb.log_artifact(artifact)  # <- 이 한 줄이면 끝\n",
        "\n",
        "        # run 종료\n",
        "        wandb.finish()\n"
      ],
      "metadata": {
        "id": "lgkVxGw47H6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation/Visualization"
      ],
      "metadata": {
        "id": "daGkPwSC7K8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvista as pv\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# 고정 컬러범위(자동으로 쓰고 싶으면 vmin=vmax=None)\n",
        "vmin, vmax = -0.5, 0.0\n",
        "\n",
        "def _to_numpy(x):\n",
        "    if torch.is_tensor(x):\n",
        "        return x.detach().cpu().numpy()\n",
        "    return np.asarray(x)\n",
        "\n",
        "def _collect_per_node_scalars(sample, device, N_expected):\n",
        "    \"\"\"\n",
        "    sample 안에서 길이 N_expected 인 1D 스칼라(또는 (N,1))들을 찾아 dict로 반환.\n",
        "    y가 (N,k)면 각 열을 y[:,i]로 분해해서 y_col_i 로 넣음.\n",
        "    \"\"\"\n",
        "    cands = {}\n",
        "\n",
        "    # x, y 우선\n",
        "    if hasattr(sample, 'y') and sample.y is not None:\n",
        "        y = sample.y.to(device)\n",
        "        if y.dim() == 1 and y.shape[0] == N_expected:\n",
        "            cands['y'] = _to_numpy(y)\n",
        "        elif y.dim() == 2 and y.shape[0] == N_expected:\n",
        "            for i in range(y.shape[1]):\n",
        "                arr = y[:, i]\n",
        "                if arr.dim() == 1:\n",
        "                    cands[f'y_col_{i}'] = _to_numpy(arr)\n",
        "                elif arr.dim() == 2 and arr.shape[1] == 1:\n",
        "                    cands[f'y_col_{i}'] = _to_numpy(arr[:, 0])\n",
        "\n",
        "    # 일반 속성들 순회\n",
        "    for name, val in vars(sample).items():\n",
        "        # 이미 처리한 y/x 제외\n",
        "        if name in ('y', 'x'):\n",
        "            continue\n",
        "        try:\n",
        "            arr = val.to(device) if torch.is_tensor(val) else val\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if torch.is_tensor(arr) or isinstance(arr, np.ndarray):\n",
        "            arr_np = _to_numpy(arr)\n",
        "            if arr_np.ndim == 1 and arr_np.shape[0] == N_expected:\n",
        "                cands[name] = arr_np\n",
        "            elif arr_np.ndim == 2 and arr_np.shape[0] == N_expected and arr_np.shape[1] == 1:\n",
        "                cands[name] = arr_np[:, 0]\n",
        "\n",
        "        # dict 스타일(node_data 등)\n",
        "        if isinstance(val, dict):\n",
        "            for k, v in val.items():\n",
        "                arr2 = v.to(device) if torch.is_tensor(v) else v\n",
        "                if torch.is_tensor(arr2) or isinstance(arr2, np.ndarray):\n",
        "                    arr2_np = _to_numpy(arr2)\n",
        "                    if arr2_np.ndim == 1 and arr2_np.shape[0] == N_expected:\n",
        "                        cands[f'{name}.{k}'] = arr2_np\n",
        "                    elif arr2_np.ndim == 2 and arr2_np.shape[0] == N_expected and arr2_np.shape[1] == 1:\n",
        "                        cands[f'{name}.{k}'] = arr2_np[:, 0]\n",
        "\n",
        "    return cands\n",
        "\n",
        "def _select_gt_pressure(cands, p_pred):\n",
        "    \"\"\"\n",
        "    이름 매칭(press|pressure|^p$) 우선,\n",
        "    없으면 |corr| 최대 후보 선택.\n",
        "    \"\"\"\n",
        "    if not cands:\n",
        "        raise RuntimeError(\"샘플에서 노드별 스칼라 후보를 찾지 못했습니다.\")\n",
        "\n",
        "    # 1) 이름 우선 매칭\n",
        "    pat = re.compile(r'(?:^|[_.-])(p|press|pressure)(?:$|[_.-])', re.IGNORECASE)\n",
        "    name_matches = [k for k in cands.keys() if pat.search(k)]\n",
        "    if len(name_matches) == 1:\n",
        "        k = name_matches[0]\n",
        "        print(f\"GT pressure chosen by name: '{k}'\")\n",
        "        return cands[k], k\n",
        "    elif len(name_matches) > 1:\n",
        "        # 이름 후보가 여러 개라면, corr로 좁힘\n",
        "        name_matches = name_matches\n",
        "\n",
        "    # 2) 상관계수 최대(|r|)\n",
        "    keys = name_matches if name_matches else list(cands.keys())\n",
        "    def _corr(a, b):\n",
        "        a = np.asarray(a).reshape(-1)\n",
        "        b = np.asarray(b).reshape(-1)\n",
        "        if a.std() < 1e-12 or b.std() < 1e-12:\n",
        "            return 0.0\n",
        "        return float(np.corrcoef(a, b)[0, 1])\n",
        "\n",
        "    scores = [(k, _corr(cands[k], p_pred)) for k in keys]\n",
        "    scores_sorted = sorted(scores, key=lambda x: abs(x[1]), reverse=True)\n",
        "    best_k, best_r = scores_sorted[0]\n",
        "    print(\"Candidate correlations with p_pred (top 5):\")\n",
        "    for k, r in scores_sorted[:5]:\n",
        "        print(f\"  {k:>24s} : r = {r:+.4f}\")\n",
        "    print(f\"GT pressure chosen by correlation: '{best_k}' (|r|={abs(best_r):.4f})\")\n",
        "    return cands[best_k], best_k\n"
      ],
      "metadata": {
        "id": "dijPIjEe7M-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvista as pv\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import time\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "# Use existing globals if set, otherwise compute defaults later\n",
        "vmin = globals().get('vmin', None)\n",
        "vmax = globals().get('vmax', None)\n",
        "\n",
        "# Helper to convert tensors/arrays to numpy\n",
        "def _to_numpy(x):\n",
        "    if torch.is_tensor(x):\n",
        "        return x.detach().cpu().numpy()\n",
        "    return np.asarray(x)\n",
        "\n",
        "\n",
        "# --- MODIFIED INFERENCE CODE ---\n",
        "\n",
        "# Assuming val_concat was created from a list of Data objects (patches)\n",
        "# We need to perform inference on these patches individually or in batches.\n",
        "# Reuse the val_loader created earlier, which already handles batching.\n",
        "\n",
        "all_preds = []\n",
        "all_nodes = []\n",
        "\n",
        "print(\"Starting batched inference on validation patches...\")\n",
        "start = time.time()\n",
        "\n",
        "# Ensure model is in eval mode and on the correct device\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device) # Ensure model is on device before inference\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(val_loader):\n",
        "        batch = batch.to(device, non_blocking=True)\n",
        "\n",
        "        # Perform inference\n",
        "        out = model(batch)\n",
        "        if isinstance(out, tuple):\n",
        "            pred = out[0]\n",
        "        else:\n",
        "            pred = out\n",
        "\n",
        "        if pred.dim() == 1:\n",
        "            pred = pred.unsqueeze(1)\n",
        "\n",
        "        # Collect predictions and original node indices (if available in batch)\n",
        "        # If batch comes from DataLoader of patches, original node indices might not be preserved\n",
        "        # However, for visualization, we need to map predictions back to original mesh positions.\n",
        "        # For this example, let's assume the patches cover the relevant part of the original mesh\n",
        "        # and we can concatenate predictions directly.\n",
        "        # If patches overlap or represent subsets needing remapping, a more complex\n",
        "        # aggregation/remapping logic would be needed here.\n",
        "\n",
        "        # For simplicity and visualization, concatenate predictions directly.\n",
        "        # This assumes the order of nodes in concatenated patches matches some order in original mesh\n",
        "        # or that the visualization only needs relative positions within batches.\n",
        "        # A better approach would involve storing and using original node indices from patches.\n",
        "        all_preds.append(pred.detach().cpu()) # Move back to CPU to avoid memory issues during concatenation\n",
        "\n",
        "        # For visualization, we need the original coordinates corresponding to these predictions.\n",
        "        # Assuming batch.pos contains the positions relative to each patch or original positions\n",
        "        # We collect positions along with predictions.\n",
        "        all_nodes.append(batch.pos.detach().cpu()) # Collect positions\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f'Batched inference time: {elapsed:.3f}s')\n",
        "\n",
        "# Concatenate predictions and positions from all batches\n",
        "p_pred = torch.cat(all_preds, dim=0).numpy()[:, 0]\n",
        "coords = torch.cat(all_nodes, dim=0).numpy()\n",
        "N = coords.shape[0] # Total number of nodes across all batches\n",
        "\n",
        "# --- END MODIFIED INFERENCE CODE ---\n",
        "\n",
        "\n",
        "# Assuming 'sample' here refers conceptually to the full validation graph\n",
        "# from which val_patches were generated, for getting ground truth.\n",
        "# If you need GT for each batch, you'd access batch.y inside the loop.\n",
        "# For visualization of the *full* validation set prediction, we need the\n",
        "# GT for the corresponding nodes.\n",
        "\n",
        "# To get GT for visualization of the full val set, we need the original val_graphs\n",
        "# and find the nodes that correspond to the collected coords.\n",
        "# This requires a mapping which is not directly available from the DataLoader batch.\n",
        "# A simpler approach for visualization is to plot one of the original val_graphs\n",
        "# and its prediction obtained by running inference on its corresponding patches.\n",
        "\n",
        "# Let's visualize the prediction and ground truth for the *first* graph in the validation set\n",
        "# by running inference on its patches and then reassembling.\n",
        "\n",
        "# --- Visualization for the first validation graph ---\n",
        "if val_graphs:\n",
        "    first_val_graph = val_graphs[0]\n",
        "    # Find patches belonging to the first graph\n",
        "    first_graph_patches = [p for p in val_patches if hasattr(p, 'graph_id') and p.graph_id == 0]\n",
        "\n",
        "    if first_graph_patches:\n",
        "        print(f\"Visualizing results for the first validation graph (ID 0) with {len(first_graph_patches)} patches...\")\n",
        "\n",
        "        # Create a DataLoader for these specific patches\n",
        "        first_graph_loader = DataLoader(\n",
        "            first_graph_patches, batch_size=BATCH_SIZE, shuffle=False,\n",
        "            pin_memory=pin_memory, num_workers=0, persistent_workers=False\n",
        "        )\n",
        "\n",
        "        patch_preds = []\n",
        "        patch_nodes_pos = []\n",
        "        patch_nodes_original_indices = [] # We need original indices to map predictions back to the full graph\n",
        "\n",
        "        # Re-process patches to include original node indices if not already present\n",
        "        # (Assuming original nodes are 0 to N-1 in the source graph)\n",
        "        # If your patch generation already stores original indices, use them.\n",
        "        # Otherwise, you might need to modify make_khop_patches or build_khop_patch_dataset\n",
        "        # to store the 'subset' tensor (original indices) in each patch Data object.\n",
        "\n",
        "        # For demonstration, let's assume patch.original_nodes contains the indices in the *original* graph\n",
        "        # If not, you would need to modify patch creation or find another way to map.\n",
        "        # Example modification during patch creation: p.original_nodes = subset\n",
        "\n",
        "        # Assuming patches have an 'original_nodes' attribute containing indices from the full graph\n",
        "        # If not, this part needs adjustment based on how patches were created.\n",
        "        if not hasattr(first_graph_patches[0], 'original_nodes'):\n",
        "             print(\"WARNING: Patches do not have 'original_nodes' attribute. Visualization mapping might be incorrect.\")\n",
        "             print(\"Consider adding 'p.original_nodes = subset' in make_khop_patches.\")\n",
        "             # Fallback: Use concatenated positions, assuming they are ordered correctly\n",
        "             # This is less reliable for overlapping patches.\n",
        "             all_patched_pos = torch.cat([p.pos.detach().cpu() for p in first_graph_patches], dim=0).numpy()\n",
        "             # Create a dummy mapping based on concatenation order - USE WITH CAUTION\n",
        "             unique_pos, inverse_indices = np.unique(all_patched_pos, axis=0, return_inverse=True)\n",
        "             # This fallback is likely incorrect for complex patching schemes.\n",
        "             # A robust solution requires storing original node indices in patches.\n",
        "\n",
        "        collected_preds = torch.zeros(first_val_graph.num_nodes, pred.size(1), device='cpu')\n",
        "        # Assuming average prediction for overlapping nodes\n",
        "        node_counts = torch.zeros(first_val_graph.num_nodes, 1, device='cpu')\n",
        "\n",
        "\n",
        "        model.eval() # Just in case\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(first_graph_loader):\n",
        "                 batch = batch.to(device, non_blocking=True)\n",
        "                 out = model(batch)\n",
        "                 if isinstance(out, tuple):\n",
        "                    pred = out[0]\n",
        "                 else:\n",
        "                    pred = out\n",
        "                 if pred.dim() == 1:\n",
        "                    pred = pred.unsqueeze(1)\n",
        "\n",
        "                 # Map predictions back to original graph indices\n",
        "                 # This requires the original indices of nodes within the batch\n",
        "                 # Assuming `batch.original_nodes` exists and holds the original indices\n",
        "                 if hasattr(batch, 'original_nodes'):\n",
        "                     original_indices = batch.original_nodes.cpu()\n",
        "                     collected_preds[original_indices] += pred.detach().cpu()\n",
        "                     node_counts[original_indices] += 1\n",
        "                 else:\n",
        "                     print(f\"Warning: Batch {i} missing 'original_nodes'. Cannot map predictions to original graph for visualization.\")\n",
        "                     # If no mapping, cannot accurately visualize on the full graph structure.\n",
        "                     # You might need to adjust `make_khop_patches` to store `subset`.\n",
        "                     # For now, break if mapping is impossible.\n",
        "                     collected_preds = None # Indicate mapping failed\n",
        "                     break\n",
        "\n",
        "        if collected_preds is not None:\n",
        "            # Average predictions for overlapping nodes\n",
        "            collected_preds = collected_preds / (node_counts + 1e-8)\n",
        "            p_pred_full_graph = collected_preds.numpy()[:, 0]\n",
        "            coords_full_graph = first_val_graph.pos.detach().cpu().numpy()\n",
        "            p_true_full_graph = first_val_graph.y.detach().cpu().numpy()[:, 0] # Assuming y[:,0] is pressure\n",
        "\n",
        "            # Calculate metrics on the full graph prediction\n",
        "            mae_full = float(np.mean(np.abs(p_pred_full_graph - p_true_full_graph)))\n",
        "            rmse_full = float(np.sqrt(np.mean((p_pred_full_graph - p_true_full_graph) ** 2)))\n",
        "            print(f\"[Full Graph 0] MAE: {mae_full:.6f}, RMSE: {rmse_full:.6f}\")\n",
        "\n",
        "            # Determine color limits for visualization\n",
        "            if vmin is None or vmax is None:\n",
        "                _min = float(min(p_pred_full_graph.min(), p_true_full_graph.min()))\n",
        "                _max = float(max(p_pred_full_graph.max(), p_true_full_graph.max()))\n",
        "                pad = 0.05 * (_max - _min + 1e-8)\n",
        "                vmin_, vmax_ = _min - pad, _max + pad\n",
        "            else:\n",
        "                vmin_, vmax_ = vmin, vmax\n",
        "\n",
        "\n",
        "            # PV data for the full graph\n",
        "            cloud_true_full = pv.PolyData(coords_full_graph.copy()); cloud_true_full['pressure_true'] = p_true_full_graph\n",
        "            cloud_pred_full = pv.PolyData(coords_full_graph.copy()); cloud_pred_full['pressure_pred'] = p_pred_full_graph\n",
        "\n",
        "            # 1x2 plot + save\n",
        "            pl = pv.Plotter(shape=(1, 2), off_screen=True, border=True)\n",
        "            pl.subplot(0, 0)\n",
        "            pl.add_text(f\"Ground Truth (p) [Graph 0]\", font_size=12)\n",
        "            pl.add_mesh(cloud_true_full, scalars='pressure_true', cmap='viridis', clim=(vmin_, vmax_), point_size=6)\n",
        "\n",
        "            pl.subplot(0, 1)\n",
        "            pl.add_text(\"Prediction (p) [Graph 0]\", font_size=12)\n",
        "            pl.add_mesh(cloud_pred_full, scalars='pressure_pred', cmap='viridis', clim=(vmin_, vmax_), point_size=6)\n",
        "\n",
        "            pl.link_views()\n",
        "            pl.view_isometric()\n",
        "            out_png = f\"pred_vs_true_pressure_graph0.png\"\n",
        "            pl.show(screenshot=out_png)\n",
        "            print(f\"Saved side-by-side comparison to: {out_png}\")\n",
        "\n",
        "            # (Optional) Save for ParaView\n",
        "            # cloud_true_full.save(f\"pressure_true_graph0.vtp\")\n",
        "            # cloud_pred_full.save(f\"pressure_pred_graph0.vtp\")\n",
        "\n",
        "        else:\n",
        "            print(\"Skipping visualization for Graph 0 due to missing original node mapping.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No patches found for the first validation graph (ID 0). Skipping visualization.\")\n",
        "\n",
        "else:\n",
        "    print(\"No validation graphs available to visualize.\")\n",
        "\n",
        "\n",
        "# --- Original visualization logic removed as it was causing OOM ---\n",
        "# The original code attempted to visualize a large concatenated graph,\n",
        "# which led to the OutOfMemoryError. Visualizing individual patches\n",
        "# or reassembling predictions on the original graph is necessary.\n",
        "# The code above now attempts to visualize the first full graph from its patches."
      ],
      "metadata": {
        "id": "aE7mnHKK7TDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}